# Лекція 20. Обробка великих обсягів даних (Big Data)

## Вступ

Сучасний цифровий світ генерує величезні обсяги даних щосекунди. Соціальні мережі, інтернет-магазини, сенсори IoT, фінансові транзакції, медичні записи — усі ці джерела створюють потоки інформації, які вимірюються терабайтами та петабайтами. Традиційні системи управління базами даних та підходи до обробки даних виявилися неспроможними ефективно працювати з такими обсягами. Саме тому виникла необхідність у нових технологіях та парадигмах, що отримали загальну назву Big Data.

Обробка великих даних стала критично важливою для бізнесу, науки та суспільства. Компанії використовують Big Data для аналізу поведінки клієнтів, оптимізації бізнес-процесів та прийняття стратегічних рішень. Наукові дослідження в галузях від геноміки до астрономії спираються на аналіз величезних масивів даних. Урядові організації використовують Big Data для покращення публічних послуг та виявлення шахрайства.

Ця лекція розглядає фундаментальні концепції Big Data, основні технології та підходи до обробки великих обсягів даних. Ми вивчимо характеристики, що відрізняють Big Data від традиційних даних, розглянемо розподілені обчислювальні парадигми, дослідимо екосистему Hadoop та NoSQL системи для Big Data, а також познайомимося з технологіями потокової обробки даних у реальному часі.

## Характеристики Big Data

Термін Big Data описує не просто великі обсяги даних, а набір характеристик, які роблять дані складними для обробки традиційними методами. Класично Big Data визначається через модель "3V", яка згодом розширилася до "5V" та навіть більше вимірів.

### Модель 3V: базові характеристики

Початкова концепція Big Data була сформульована через три ключові виміри, відомі як модель 3V.

**Volume (Обсяг)**

Перша та найочевидніша характеристика Big Data — це величезні обсяги інформації. Сучасні організації збирають і зберігають дані в обсягах, які вимірюються терабайтами, петабайтами, а іноді й ексабайтами.

Приклади масштабів даних у різних галузях:

Соціальні мережі генерують величезні обсяги контенту. Facebook обробляє понад 4 петабайти даних щодня, включаючи публікації, коментарі, фотографії та відео. YouTube отримує понад 500 годин відеоконтенту щохвилини. Twitter обробляє понад 500 мільйонів твітів щодня. Такі обсяги даних неможливо ефективно зберігати та обробляти в традиційних реляційних базах даних на одному сервері.

У наукових дослідженнях обсяги даних також вражають. Великий адронний колайдер у ЦЕРН генерує приблизно 30 петабайтів даних щороку. Проєкт Square Kilometre Array, радіотелескоп наступного покоління, очікується генеруватиме понад 1 ексабайт даних щодня. Геномні дослідження створюють терабайти даних для кожного великомасштабного проєкту секвенування.

Інтернет речей створює постійно зростаючі потоки даних. Сучасне місто може мати мільйони сенсорів, які відстежують трафік, якість повітря, споживання енергії та інші параметри. Промислові підприємства використовують десятки тисяч сенсорів для моніторингу обладнання. Автономні автомобілі генерують приблизно 4 терабайти даних щодня від різних сенсорів та камер.

**Velocity (Швидкість)**

Другий вимір стосується швидкості генерації, надходження та обробки даних. У багатьох випадках критично важливо обробляти дані практично в реальному часі, щоб отримати цінні інсайти або прийняти своєчасні рішення.

Фінансові ринки демонструють екстремальні вимоги до швидкості. Високочастотний трейдинг вимагає обробки мільйонів транзакцій за мілісекунди. Системи виявлення шахрайства повинні аналізувати транзакції в режимі реального часу, перш ніж вони будуть завершені. Затримка навіть у кілька мілісекунд може призвести до значних фінансових втрат.

Соціальні мережі обробляють величезні потоки даних безперервно. Під час великих подій, таких як спортивні змагання або політичні дебати, кількість публікацій, коментарів та реакцій зростає в рази. Системи повинні обробляти ці піки навантаження без затримок та збоїв. Рекомендаційні системи мають негайно реагувати на дії користувачів, щоб показувати релевантний контент.

Моніторинг інфраструктури та систем безпеки вимагає обробки потоків даних у реальному часі. Системи виявлення вторгнень аналізують мережевий трафік та журнали подій для швидкого виявлення аномалій. Моніторинг промислового обладнання відстежує показники в реальному часі для запобігання аваріям. Медичні системи моніторингу пацієнтів повинні миттєво реагувати на критичні зміни показників життєдіяльності.

**Variety (Різноманітність)**

Третій вимір описує різноманітність типів та структур даних. На відміну від традиційних реляційних баз даних, де дані зберігаються в структурованих таблицях, Big Data включає структуровані, напівструктуровані та неструктуровані дані з різних джерел.

Структуровані дані мають чітко визначену схему та зберігаються в таблицях з фіксованими полями. Це традиційні дані баз даних, таблиці Excel, CSV файли. Хоча це найпростіший тип даних для обробки, у Big Data вони становлять лише невелику частину загального обсягу.

Напівструктуровані дані мають певну організацію, але не відповідають жорсткій табличній структурі. JSON та XML документи, електронні листи, лог-файли серверів належать до цієї категорії. Ці дані містять теги, метадані або ієрархічну структуру, що полегшує їх обробку, але вимагає спеціалізованих інструментів.

Неструктуровані дані не мають попередньо визначеної структури та становлять близько 80-90% усіх даних Big Data. Текстові документи, електронні листи, публікації в соціальних мережах, відео, аудіо, зображення, дані сенсорів — усе це приклади неструктурованих даних. Для їх обробки та аналізу потрібні спеціалізовані технології, такі як обробка природної мови, комп'ютерний зір та машинне навчання.

### Розширена модель 5V

З розвитком технологій Big Data модель була розширена додатковими вимірами, що краще відображають виклики роботи з великими даними.

**Veracity (Достовірність)**

Четвертий вимір стосується якості та надійності даних. Великі обсяги даних не гарантують їх точності або корисності. Проблеми з якістю даних можуть виникати через різні причини.

Неточність та помилки в даних можуть з'являтися на етапі збору. Несправні сенсори можуть надавати некоректні показники. Користувачі можуть вводити неправильну інформацію. Системи інтеграції можуть неправильно перетворювати дані між різними форматами. Кожне з цих джерел помилок може призвести до неправильних висновків при аналізі.

Неповнота даних також представляє значну проблему. Відсутні значення в записах ускладнюють аналіз. Сенсори можуть періодично відключатися, створюючи прогалини в даних. Користувачі можуть не заповнювати всі поля форм. Методи обробки відсутніх даних, такі як імпутація або видалення, повинні застосовуватися обережно, щоб не спотворити результати аналізу.

Суперечливість між різними джерелами даних ускладнює створення єдиної картини. Різні системи можуть зберігати одну й ту саму інформацію в різних форматах. Дані можуть бути застарілими в одній системі та актуальними в іншій. Розв'язання конфліктів та забезпечення узгодженості даних з різних джерел вимагає складних процесів інтеграції та очищення.

**Value (Цінність)**

П'ятий вимір підкреслює необхідність перетворення великих обсягів даних у корисні інсайти та цінність для бізнесу або дослідження. Не всі зібрані дані є корисними, і виявлення цінної інформації в морі даних — це ключовий виклик Big Data.

Співвідношення сигналу до шуму в Big Data часто є низьким. Більшість зібраних даних можуть бути нерелевантними для конкретного аналізу. Виявлення значущих патернів серед випадкових флуктуацій вимагає складних статистичних методів. Алгоритми машинного навчання повинні бути налаштовані так, щоб фільтрувати шум та виявляти справжні закономірності.

Контекст та інтерпретація є критичними для отримання цінності з даних. Ті самі дані можуть мати різне значення в різних контекстах. Кореляції між змінними не завжди означають причинно-наслідкові зв'язки. Людська експертиза та розуміння предметної області залишаються необхідними для правильної інтерпретації результатів аналізу даних.

### Додаткові виміри

Деякі дослідники та практики пропонують розширити модель додатковими вимірами.

**Variability (Мінливість)**

Цей вимір описує непостійність значення та структури даних з часом. Значення одних і тих самих показників можуть змінюватися залежно від контексту. Наприклад, слово може мати різні значення в різних контекстах або на різних мовах. Структура даних може еволюціонувати з часом, коли додаються нові поля або змінюються формати. Такі зміни ускладнюють створення стабільних аналітичних моделей.

**Visualization (Візуалізація)**

Представлення великих обсягів даних у зрозумілій формі є критично важливим для прийняття рішень. Традиційні методи візуалізації часто не справляються з багатовимірними великими наборами даних. Інтерактивні дашборди та інструменти візуалізації дозволяють користувачам досліджувати дані та виявляти інсайти. Візуалізація також допомагає ідентифікувати проблеми якості даних та аномалії.

## Розподілені обчислювальні парадигми

Обробка великих обсягів даних вимагає розподілення обчислень по множині машин. Традиційний підхід вертикального масштабування, коли використовується один потужний сервер, стає економічно неефективним та має фізичні обмеження. Горизонтальне масштабування, коли обчислення розподіляються по кластеру звичайних серверів, стало основним підходом у Big Data. Це потребувало розробки нових парадигм програмування, які дозволяють ефективно виконувати обчислення на розподілених системах.

### MapReduce

Парадигма MapReduce, розроблена Google та опублікована в 2004 році, стала революційним підходом до обробки великих даних на кластерах машин. Вона надає простий, але потужний спосіб паралельної обробки даних, приховуючи складність розподіленості від програміста.

**Концептуальна модель MapReduce**

MapReduce розбиває обчислення на дві основні фази: Map та Reduce. Ця ідея запозичена з функціонального програмування, де map та reduce є стандартними операціями над колекціями.

Фаза Map приймає вхідні дані та перетворює їх у набір проміжних пар ключ-значення. Кожна Map-задача обробляє частину вхідних даних незалежно від інших. Функція map застосовується до кожного елемента вхідних даних, генеруючи нуль або більше проміжних пар. Наприклад, при підрахунку слів у текстових документах функція map отримує рядок тексту та генерує пари, де ключем є слово, а значенням число один.

Між фазами Map та Reduce відбувається процес shuffle and sort. Система автоматично групує всі проміжні значення за ключами та розподіляє їх між Reduce-задачами. Усі значення з одним ключем потрапляють до однієї Reduce-задачі. Цей етап є критично важливим для правильності результату та часто є найбільш ресурсоємною частиною обчислення.

Фаза Reduce приймає ключ та набір усіх значень, асоційованих з цим ключем, та виконує агрегацію або іншу обробку. Функція reduce зазвичай підсумовує, знаходить максимум, конкатенує або виконує інші операції над набором значень. Результатом є фінальні пари ключ-значення, які записуються в розподілену файлову систему.

**Приклад задачі підрахунку слів**

Класичний приклад MapReduce — підрахунок частоти слів у великій колекції документів. Це проста, але наочна задача, яка демонструє основні концепції парадигми.

Уявімо, що маємо мільйони текстових документів, розподілених по кластеру серверів. Вхідні дані розбиваються на блоки, кожен з яких обробляється окремою Map-задачею незалежно від інших. Функція map отримує блок тексту, розбиває його на слова та для кожного слова генерує пару, де ключ — це слово, а значення — число один.

Проміжні результати виглядають як набір пар. Слово "databases" з'являється багато разів у різних частинах вхідних даних, кожне входження генерує пару з ключем "databases" та значенням один. Після фази Map система має безліч таких пар, розкиданих по різних машинах.

Shuffle and sort збирає всі пари з однаковим ключем разом. Усі пари з ключем "databases" групуються та передаються одній Reduce-задачі. Те ж саме відбувається для всіх інших слів. Таким чином, кожна Reduce-задача отримує ключ та список усіх значень для цього ключа.

Функція reduce просто підсумовує всі значення для кожного ключа. Для ключа "databases" це може бути список із тисяч одиниць. Reduce підсумовує їх та записує фінальний результат: пару з ключем "databases" та значенням, що дорівнює загальній кількості входжень цього слова в усіх документах.

**Переваги та обмеження MapReduce**

MapReduce має кілька важливих переваг. Простота програмної моделі дозволяє розробникам зосередитися на логіці обробки даних, не турбуючись про складність розподіленості. Автоматичне паралелізування означає, що система сама розподіляє задачі по машинах кластера. Відмовостійкість вбудована в фреймворк: якщо якась задача завершується з помилкою, система автоматично перезапускає її на іншій машині. Масштабованість дозволяє обробляти петабайти даних, додаючи більше машин до кластера.

Проте MapReduce має і значні обмеження. Затримки виконання можуть бути високими через запис проміжних результатів на диск. Неефективність для ітеративних алгоритмів означає, що багато алгоритмів машинного навчання, які вимагають багаторазового проходу по даних, працюють повільно. Обмежена виразність дворівневої моделі Map-Reduce ускладнює реалізацію складної логіки обробки. Висока складність для інтерактивних запитів робить MapReduce непридатним для завдань, що вимагають швидкої відповіді.

### Apache Spark

Apache Spark був розроблений у Каліфорнійському університеті в Берклі як відповідь на обмеження MapReduce. Випущений у 2014 році, Spark швидко став одним з найпопулярніших фреймворків для обробки Big Data завдяки своїй швидкості та зручності.

**Архітектура та основні концепції**

Spark базується на концепції Resilient Distributed Datasets — розподілених колекцій об'єктів, які можуть оброблятися паралельно. RDD є основною абстракцією даних у Spark. RDD можуть створюватися з файлів у розподіленій файловій системі, з колекцій у пам'яті драйвера, або шляхом трансформації існуючих RDD.

Ключова особливість Spark — обробка даних у пам'яті. Замість запису проміжних результатів на диск після кожної операції, як це робить MapReduce, Spark зберігає дані в пам'яті між операціями. Це значно прискорює виконання, особливо для ітеративних алгоритмів, які багаторазово обробляють ті самі дані. Для наборів даних, які не поміщаються в пам'ять, Spark інтелектуально керує пам'яттю та може використовувати диск при необхідності.

Spark підтримує два типи операцій над RDD. Трансформації створюють нові RDD з існуючих, наприклад map, filter, join. Трансформації є ледачими: вони не виконуються відразу, а лише будують граф обчислень. Дії запускають обчислення та повертають результат драйверу або записують у зовнішню систему, наприклад count, collect, saveAsTextFile.

Spark автоматично оптимізує виконання обчислень. Коли викликається дія, Spark аналізує граф трансформацій та створює оптимізований план виконання. Він може об'єднувати кілька трансформацій в одну операцію, мінімізувати переміщення даних по мережі та виконувати інші оптимізації.

**Екосистема Spark**

Spark не просто фреймворк обробки даних, а ціла екосистема інструментів для різних завдань.

Spark SQL надає інтерфейс для роботи з структурованими даними за допомогою SQL та DataFrame API. DataFrame — це розподілена колекція даних, організована в іменовані стовпці, схожа на таблицю в реляційній базі даних. Spark SQL може читати дані з різних джерел: Parquet, JSON, Hive, JDBC та інших. Catalyst optimizer автоматично оптимізує запити, забезпечуючи високу продуктивність.

Spark Streaming дозволяє обробляти потоки даних у реальному часі. Він розбиває потік на мікробатчі та обробляє їх за допомогою Spark engine. Structured Streaming, новіша версія, надає більш високорівневий API та гарантії доставки. Spark Streaming може інтегруватися з Kafka, Flume, Kinesis та іншими системами потокової обробки.

MLlib — це бібліотека машинного навчання для Spark. Вона включає реалізації популярних алгоритмів класифікації, регресії, кластеризації та колаборативної фільтрації. Алгоритми оптимізовані для роботи з великими розподіленими наборами даних. MLlib підтримує як низькорівневий RDD API, так і високорівневий DataFrame-based API.

GraphX — це бібліотека для обробки графів та паралельних обчислень на графах. Вона включає алгоритми для PageRank, виявлення компонент зв'язності, обчислення найкоротших шляхів та інші. GraphX об'єднує переваги графових обчислень та обробки даних у Spark.

**Порівняння Spark та MapReduce**

Spark значно перевершує MapReduce за швидкістю для більшості завдань. Для ітеративних алгоритмів, таких як машинне навчання, Spark може бути в 10-100 разів швидшим завдяки обробці в пам'яті. Навіть для простих завдань обробки даних Spark часто швидший у 2-10 разів.

Зручність розробки в Spark також вища. Він підтримує інтерактивні оболонки для Scala, Python та R, що дозволяє швидко експериментувати з даними. Високорівневі API, такі як DataFrame та Dataset, роблять код більш читабельним та менш схильним до помилок. Єдина платформа для батч-обробки, потокової обробки, SQL та машинного навчання спрощує архітектуру додатків.

MapReduce все ще може бути корисним для деяких сценаріїв. Він має нижчу вартість входження для простих завдань. Для дуже великих наборів даних, які не поміщаються в пам'ять кластера, MapReduce може бути більш ресурсоефективним. Він також може бути кращим вибором, коли важлива максимальна стабільність та передбачуваність виконання.

## Hadoop екосистема

Apache Hadoop — це фреймворк з відкритим кодом для розподіленого зберігання та обробки великих наборів даних на кластерах звичайних серверів. Розроблений під впливом статей Google про MapReduce та Google File System, Hadoop став де-факто стандартом для Big Data обробки протягом багатьох років.

### HDFS — Hadoop Distributed File System

HDFS є серцем екосистеми Hadoop, надаючи надійне розподілене зберігання даних. Він розроблений спеціально для роботи з дуже великими файлами на звичайному обладнанні.

**Архітектура HDFS**

HDFS використовує архітектуру master-slave з одним NameNode та множиною DataNode. NameNode є головним сервером, який управляє простором імен файлової системи та регулює доступ до файлів від клієнтів. Він зберігає метадані про файли та блоки: яким файлам належать які блоки, на яких DataNode зберігаються реплікації кожного блоку. NameNode зберігає всі ці метадані в оперативній пам'яті для швидкого доступу.

DataNode зберігають фактичні блоки даних. Кожен файл розбивається на блоки фіксованого розміру, зазвичай 128 MB або 256 MB. Кожен блок реплікується на кілька DataNode для забезпечення надійності, типово на три вузли. DataNode періодично відправляють heartbeat повідомлення до NameNode, щоб підтвердити, що вони функціонують. Якщо NameNode не отримує heartbeat від DataNode протягом певного часу, він вважає його неактивним та переміщує блоки на інші вузли.

Secondary NameNode не є резервною копією NameNode, як можна подумати з назви. Він періодично об'єднує edit log з поточним image файлом NameNode для зменшення часу запуску. У випадку відмови NameNode відновлення може зайняти значний час, тому в критичних системах використовують High Availability конфігурацію з двома NameNode.

**Принципи роботи HDFS**

HDFS оптимізована для послідовного читання великих файлів. Клієнт, який хоче прочитати файл, спочатку звертається до NameNode, щоб дізнатися розташування блоків. NameNode повертає список DataNode, на яких зберігаються блоки файлу, впорядкований за близькістю до клієнта. Потім клієнт безпосередньо читає дані з DataNode, не навантажуючи NameNode передачею даних.

Запис у HDFS також оптимізований для потокового запису великих файлів. Коли клієнт створює файл, NameNode створює запис у простору імен без блоків. Коли клієнт записує дані, вони спочатку буферизуються локально. Коли буфер заповнюється, клієнт запитує у NameNode список DataNode для реплікації першого блоку. Клієнт відправляє дані першому DataNode, який пересилає їх другому, а той третьому, створюючи pipeline реплікації.

HDFS не підтримує модифікацію існуючих файлів, окрім дописування в кінець. Файли є write-once, read-many. Це спрощує архітектуру та забезпечує консистентність даних. Для додатків, які потребують випадкового запису або оновлення, використовуються інші системи зберігання, такі як HBase.

**Переваги та обмеження HDFS**

HDFS надає високу пропускну здатність для послідовного читання та запису великих файлів. Відмовостійкість досягається через реплікацію блоків. Масштабованість дозволяє зберігати петабайти даних, додаючи нові вузли. Простота використання досягається через знайомий файловий інтерфейс.

Проте HDFS має обмеження. Висока затримка доступу до даних робить його непридатним для завдань реального часу. Неефективність для малих файлів означає, що мільйони малих файлів можуть перевантажити NameNode. Єдина точка відмови NameNode у стандартній конфігурації вимагає спеціальних заходів для високої доступності.

### YARN — Yet Another Resource Negotiator

YARN є системою управління ресурсами в Hadoop 2.0 та пізніших версіях. Він відокремлює функціональність управління ресурсами від програмної моделі MapReduce, дозволяючи різним фреймворкам обробки даних працювати на одному кластері.

**Архітектура YARN**

ResourceManager є головним демоном YARN, який управляє розподілом ресурсів по всьому кластеру. Він отримує запити на ресурси від додатків та розподіляє їх на основі доступності та політик. ResourceManager складається з двох основних компонентів: Scheduler, який відповідає за розподіл ресурсів між запущеними додатками, та ApplicationsManager, який приймає подання нових додатків та переговорює про контейнери для запуску ApplicationMaster.

NodeManager працює на кожному робочому вузлі кластера. Він відповідає за запуск та моніторинг контейнерів на своєму вузлі. Контейнер — це абстракція ресурсів: певна кількість CPU, пам'яті та можливо інших ресурсів. NodeManager повідомляє ResourceManager про доступні ресурси та стан контейнерів на своєму вузлі.

ApplicationMaster — це специфічний для фреймворку процес, який керує виконанням конкретного додатка. Кожен додаток має свій власний ApplicationMaster, який переговорює з ResourceManager про ресурси та працює з NodeManager для виконання та моніторингу завдань. Для MapReduce додатків ApplicationMaster керує життєвим циклом Map та Reduce задач. Для Spark додатків він координує виконання Spark jobs.

**Переваги YARN**

YARN дозволяє кільком фреймворкам обробки даних спільно використовувати один кластер. MapReduce, Spark, Flink та інші можуть працювати разом, ефективно розподіляючи ресурси. Це підвищує утилізацію кластера та знижує операційні витрати.

Гнучкість у розподілі ресурсів дозволяє налаштовувати пріоритети додатків та гарантії ресурсів. Scheduler у YARN підтримує різні політики: CapacityScheduler, FairScheduler та інші. Масштабованість YARN дозволяє підтримувати кластери з тисячами вузлів.

### Компоненти екосистеми Hadoop

Навколо Hadoop виникла багата екосистема інструментів для різних аспектів роботи з Big Data.

**Apache Hive**

Hive надає SQL-подібний інтерфейс для запитів до даних у Hadoop. Він транслює HiveQL запити в MapReduce або Spark jobs. Hive включає метасховище, яке зберігає схеми таблиць та партицій. Це дозволяє аналітикам працювати з Big Data без необхідності писати MapReduce код.

Hive добре підходить для batch analytics та ETL процесів. Він підтримує складні типи даних, партиціонування для оптимізації запитів, та інтеграцію з BI інструментами. Проте Hive має високу затримку, що робить його непридатним для інтерактивних запитів.

**Apache Pig**

Pig надає високорівневу мову Pig Latin для обробки даних. На відміну від декларативного SQL у Hive, Pig Latin є процедурною мовою, яка дозволяє більше контролю над потоком обробки. Pig транслює Pig Latin скрипти в послідовність MapReduce jobs.

Pig добре підходить для ETL процесів, де потрібна складна логіка трансформації даних. Він має багату бібліотеку функцій та підтримує користувацькі функції на Java, Python та інших мовах.

**Apache HBase**

HBase є розподіленою NoSQL базою даних, побудованою поверх HDFS. Вона надає випадковий доступ для читання та запису до великих наборів даних. HBase моделює дані як розріджені, багатовимірні сортовані мапи. Вона використовується для сценаріїв, які вимагають швидкого доступу до окремих записів у дуже великих таблицях.

HBase автоматично шардує таблиці на регіони та розподіляє їх по серверах. Вона забезпечує сильну консистентність для операцій читання та запису. HBase інтегрується з MapReduce та Spark для batch обробки та з Phoenix для SQL доступу.

**Apache Sqoop**

Sqoop дозволяє ефективно переносити дані між Hadoop та реляційними базами даних. Він може імпортувати таблиці або результати запитів з MySQL, PostgreSQL, Oracle та інших RDBMS в HDFS або Hive. Sqoop також може експортувати дані з Hadoop назад у реляційні бази.

Sqoop використовує MapReduce для паралелізації передачі даних, що забезпечує високу пропускну здатність. Він підтримує інкрементальний імпорт для періодичного оновлення даних у Hadoop.

**Apache Flume та Apache Kafka**

Flume та Kafka використовуються для збору та передачі потоків даних у Hadoop. Flume є розподіленою системою для ефективного збору, агрегації та переміщення великих обсягів лог-даних. Він має простий та гнучкий дизайн на основі потокових даних.

Kafka — це розподілена платформа потокової обробки, яка може обробляти трильйони подій на день. Вона надає більш загальну модель pub-sub messaging та зберігає повідомлення на диску для довговічності. Kafka часто використовується як буфер між джерелами даних та системами обробки.

## NoSQL системи для Big Data

Реляційні бази даних, незважаючи на свою потужність та універсальність, стикаються з обмеженнями при роботі з Big Data. Вимоги до горизонтального масштабування, роботи з різноманітними типами даних та обробки великих обсягів записів призвели до розвитку NoSQL баз даних.

### Cassandra

Apache Cassandra — це розподілена NoSQL база даних, розроблена для обробки великих обсягів даних на багатьох серверах без єдиної точки відмови. Вона поєднує архітектуру Dynamo від Amazon та модель даних BigTable від Google.

**Архітектура Cassandra**

Cassandra використовує архітектуру peer-to-peer без головного вузла. Усі вузли в кластері рівноправні та виконують однакові функції. Це забезпечує високу доступність та відсутність єдиної точки відмови. Клієнт може підключитися до будь-якого вузла, який стає координатором для цього запиту.

Дані розподіляються по вузлах за допомогою консистентного хешування. Кожен вузол відповідає за певний діапазон хеш-значень. Partition key визначає, на яких вузлах зберігатиметься рядок. Replication factor визначає, скільки копій кожного рядка зберігається. Реплікація здійснюється на основі стратегії розміщення, яка враховує топологію датацентрів та rack.

Cassandra забезпечує настроювану консистентність через параметри рівня консистентності для операцій читання та запису. Рівень ONE вимагає підтвердження від одного вузла, забезпечуючи найнижчу затримку. Рівень QUORUM вимагає підтвердження від більшості реплік. Рівень ALL вимагає підтвердження від усіх реплік, забезпечуючи найсильнішу консистентність ціною вищої затримки.

**Модель даних Cassandra**

Cassandra використовує широкостовпчикову модель даних. Дані організовані в таблиці, які складаються з рядків та стовпців. Кожен рядок ідентифікується partition key, який може бути складеним з кількох стовпців. Clustering columns визначають порядок сортування даних всередині партиції.

Cassandra Query Language надає SQL-подібний синтаксис для взаємодії з базою даних. Проте CQL має обмеження порівняно з SQL. Запити повинні бути ефективними та зазвичай фільтрують за partition key. Joins та складні агрегації не підтримуються. Модель даних повинна проектуватися з урахуванням запитів, які будуть виконуватися.

**Випадки використання Cassandra**

Cassandra відмінно підходить для сценаріїв з великими обсягами записів. Системи телеметрії, моніторингу та логування генерують величезні потоки даних, які Cassandra може ефективно обробляти. Завдяки лінійній масштабованості продуктивність збільшується пропорційно кількості вузлів.

Часові ряди даних — це природна область застосування Cassandra. Дані з сенсорів, метрики додатків, фінансові дані часто організовані як часові ряди. Clustering columns дозволяють ефективно зберігати та запитувати дані за часовими діапазонами.

Глобально розподілені додатки використовують мультидатацентрову реплікацію Cassandra. Можна налаштувати кластер для реплікації даних між географічно розподіленими датацентрами. Це забезпечує низьку затримку для користувачів по всьому світу та високу доступність при відмові датацентру.

### HBase

Apache HBase — це розподілена, масштабована NoSQL база даних, побудована поверх HDFS. Вона моделює дані як розріджену, багатовимірну, сортовану мапу. HBase надає випадковий доступ для читання та запису до окремих рядків у дуже великих таблицях.

**Архітектура HBase**

HBase використовує master-slave архітектуру. HMaster координує кластер HBase, управляє схемою таблиць та розподілом регіонів. RegionServer обслуговують один або більше регіонів, які є горизонтальними розділами таблиць. Кожен регіон містить множину стовпчикових сімейств.

ZooKeeper використовується для координації розподіленої системи. Він зберігає інформацію про те, який RegionServer обслуговує які регіони, відстежує стан серверів та забезпечує консистентність метаданих. Клієнти взаємодіють з ZooKeeper, щоб знайти потрібний RegionServer.

Дані в HBase зберігаються в HDFS у форматі HFile. Кожен HFile містить відсортовані key-value пари для певного регіону та стовпчикової сімейства. Write-Ahead Log використовується для забезпечення довговічності записів. Усі зміни спочатку записуються в WAL, перш ніж застосовуватися до MemStore.

**Модель даних HBase**

Таблиці в HBase складаються з рядків та стовпців. Кожен рядок ідентифікується унікальним row key, який є впорядкованим байтовим масивом. Стовпці групуються в стовпчикові сімейства, які визначаються при створенні таблиці. Усередині стовпчикової сімейства можна динамічно додавати стовпці.

Кожна комірка в HBase може зберігати кілька версій значення з різними часовими мітками. За замовчуванням зберігається остання версія, але це налаштовується. Версіонування дозволяє відстежувати історію змін даних.

HBase підтримує сильну консистентність для операцій читання та запису в межах одного рядка. Атомарні операції read-modify-write забезпечують коректність при паралельному доступі. Проте HBase не підтримує транзакції між кількома рядками.

**Випадки використання HBase**

HBase використовується для сценаріїв, які вимагають швидкого випадкового доступу до окремих записів у дуже великих таблицях. Системи реального часу, такі як месенджери та соціальні мережі, використовують HBase для зберігання повідомлень, профілів користувачів та графів зв'язків.

Системи рекомендацій та персоналізації зберігають великі обсяги даних про користувачів та їх поведінку в HBase. Швидкий доступ до профілю користувача дозволяє генерувати персоналізовані рекомендації в реальному часі.

Збір та аналіз великих обсягів часових рядів даних також є областю застосування HBase. Моніторинг ІТ-інфраструктури, телеметрія IoT пристроїв, логи застосунків зберігаються в HBase для швидкого доступу та аналізу.

## Потокова обробка даних

Традиційна batch обробка, коли дані накопичуються та обробляються періодично, не задовольняє потреби багатьох сучасних додатків. Системи фінансового трейдингу, моніторингу безпеки, рекомендаційні системи та інші вимагають обробки даних у реальному часі або майже реальному часі. Потокова обробка дозволяє аналізувати дані, коли вони надходять, забезпечуючи низьку затримку від події до інсайту.

### Apache Kafka

Apache Kafka — це розподілена платформа потокової обробки, розроблена LinkedIn та випущена як open source у 2011 році. Kafka може обробляти трильйони подій на день та є критичною інфраструктурою для багатьох великих організацій.

**Архітектура Kafka**

Kafka організована навколо концепції topic — це категорія або потік записів. Виробники публікують повідомлення в топіки, а споживачі підписуються на топіки та обробляють повідомлення. Кожен топік розділений на партиції, які є впорядкованими, незмінними послідовностями повідомлень. Партиції дозволяють паралелізувати обробку та масштабувати систему.

Kafka кластер складається з кількох broker серверів. Кожна партиція має лідера та нуль або більше слідувачів. Лідер обробляє всі запити читання та запису для партиції, а слідувачі реплікують дані лідера. Якщо лідер відмовляє, один зі слідувачів автоматично стає новим лідером.

ZooKeeper використовується для координації Kafka кластера. Він зберігає метадані про брокери, топіки та партиції, координує вибори лідерів та відстежує стан споживачів. У новіших версіях Kafka ZooKeeper поступово замінюється внутрішнім механізмом consensus.

**Гарантії доставки Kafka**

Kafka надає гнучкі гарантії доставки повідомлень. At-most-once delivery означає, що повідомлення можуть бути втрачені, але ніколи не доставляються двічі. Це найшвидший режим, але ризикований для критичних даних. At-least-once delivery гарантує, що повідомлення не будуть втрачені, але можуть бути доставлені кілька разів. Споживачі повинні бути ідемпотентними або дедуплікувати повідомлення. Exactly-once delivery гарантує, що кожне повідомлення доставляється та обробляється рівно один раз. Це досягається через транзакції та ідемпотентність виробників.

**Kafka Streams**

Kafka Streams — це бібліотека для побудови додатків потокової обробки. На відміну від окремих фреймворків, таких як Storm чи Flink, Kafka Streams є просто бібліотекою Java, яка працює в контексті вашого додатка. Це спрощує розгортання та управління.

Kafka Streams надає високорівневі операції для обробки потоків: фільтрація, трансформація, агрегація, об'єднання потоків. Він підтримує stateful обробку з локальними сховищами станів, які автоматично реплікуються для відмовостійкості. Exactly-once processing semantics забезпечує коректність результатів.

**Випадки використання Kafka**

Kafka широко використовується як центральна шина даних в організаціях. Мікросервіси публікують події про зміни стану в Kafka, а інші сервіси підписуються на релевантні події. Це забезпечує слабку зв'язаність між компонентами системи.

Реал-тайм ETL процеси використовують Kafka для передачі даних між системами. Дані з операційних баз даних, логи застосунків, клікстріми користувачів потрапляють в Kafka та передаються в аналітичні системи, сховища даних або озера даних.

Моніторинг та алертинг систем збирають метрики та події з різних джерел через Kafka. Потокова обробка аналізує ці дані в реальному часі для виявлення аномалій, перевищення порогів та інших подій, що вимагають негайної реакції.

### Apache Storm

Apache Storm — це розподілена система потокової обробки реального часу. Вона обробляє необмежені потоки даних, гарантуючи обробку кожного повідомлення принаймні один раз.

**Архітектура Storm**

Storm topology — це граф обробки даних. Він складається з spout, які читають дані з джерел, та bolt, які обробляють дані. Spout генерують потік tuples, а bolts їх трансформують, фільтрують, агрегують або записують у зовнішні системи.

Nimbus — це головний вузол Storm кластера, схожий на JobTracker у Hadoop. Він розподіляє код по кластеру, призначає завдання робочим вузлам та моніторить виконання. Supervisor управляють робочими процесами на кожному вузлі. ZooKeeper координує кластер та зберігає стан.

Storm гарантує at-least-once processing через механізм ack/fail. Коли tuple успішно обробляється, bolt відправляє acknowledgment. Якщо tuple не обробляється протягом timeout, він автоматично перезапускається. Для exactly-once семантики використовується Trident, високорівневий API поверх Storm.

**Випадки використання Storm**

Реал-тайм аналітика потребує негайної обробки даних. Моніторинг соціальних мереж для виявлення трендів, аналіз клікстрімів користувачів для персоналізації, обробка телеметрії IoT для виявлення аномалій — усе це сценарії для Storm.

Continuous computation дозволяє постійно оновлювати результати аналізу. Наприклад, топ-trending topics у Twitter оновлюються в реальному часі на основі потоку твітів. Storm підтримує такі постійно працюючі обчислення.

### Apache Flink

Apache Flink — це фреймворк та механізм розподілених обчислень для stateful обробки необмежених та обмежених потоків даних. Flink може виконувати як потокову обробку, так і batch обробку, розглядаючи batch як спеціальний випадок потокової обробки.

**Особливості Flink**

Flink підтримує справжню потокову обробку, на відміну від мікробатч підходу Spark Streaming. Це забезпечує нижчу затримку та більш точну обробку в реальному часі. Event time processing дозволяє коректно обробляти події навіть при затримках або невпорядкованому надходженні.

Stateful обробка з exactly-once гарантіями є ключовою особливістю Flink. Він підтримує складні stateful операції, такі як вікна, приєднання потоків та pattern matching. Механізм checkpointing автоматично зберігає стан додатка та може відновлюватися після збоїв без втрати даних або дублювання.

Flink має багаті API для різних рівнів абстракції. DataStream API надає низькорівневий контроль над потоковою обробкою. Table API та SQL дозволяють використовувати декларативні запити для потоків. ProcessFunction API надає максимальну гнучкість для складної логіки.

**Порівняння систем потокової обробки**

Kafka Streams найкраще підходить для простих трансформацій та аналізу даних, які вже в Kafka. Його перевага — простота розгортання як частини додатка. Проте він обмежений обробкою даних з Kafka та має менш розвинені можливості для складної stateful логіки.

Storm надає низьку затримку та простоту використання. Він добре підходить для простих потокових обчислень та має великий екосистему сполучників. Недоліком є обмежена підтримка stateful обробки та складність досягнення exactly-once семантики.

Spark Streaming добре інтегрується з екосистемою Spark та підходить для випадків, коли потрібна як batch, так і потокова обробка на одній платформі. Structured Streaming надає високорівневий API. Проте мікробатч архітектура призводить до вищої затримки порівняно зі справжніми потоковими системами.

Flink надає найбільш повні можливості для складної stateful потокової обробки з exactly-once гарантіями та низькою затримкою. Він підходить для критичних бізнес-застосунків, де важлива коректність та продуктивність. Складність Flink може бути надмірною для простих сценаріїв.

## Висновки

Обробка великих обсягів даних стала критично важливою компетенцією для сучасних організацій. Big Data характеризується не лише обсягом, але й швидкістю генерації, різноманітністю типів, проблемами достовірності та необхідністю виявлення цінності. Традиційні підходи до зберігання та обробки даних виявилися неспроможними впоратися з цими викликами.

Розподілені обчислювальні парадигми, такі як MapReduce та Spark, дозволяють ефективно обробляти величезні обсяги даних на кластерах звичайних серверів. MapReduce запропонував простий, але потужний спосіб паралельної обробки даних. Spark значно покращив продуктивність завдяки обробці в пам'яті та надав більш зручний API.

Екосистема Hadoop надає комплексне рішення для зберігання та обробки Big Data. HDFS забезпечує надійне розподілене зберігання, YARN керує ресурсами кластера, а численні компоненти, такі як Hive, Pig, HBase, вирішують специфічні завдання обробки даних.

NoSQL системи, такі як Cassandra та HBase, вирішують проблеми масштабування та різноманітності даних, які складні для традиційних реляційних баз даних. Вони жертвують деякими властивостями ACID транзакцій заради доступності та масштабованості відповідно до теореми CAP.

Потокова обробка даних дозволяє аналізувати інформацію в реальному часі або майже реальному часі. Kafka надає надійну платформу для передачі потоків даних, Storm та Flink забезпечують потужні можливості для їх обробки. Вибір конкретної технології залежить від вимог до затримки, складності обробки та гарантій доставки.

Розуміння цих технологій та парадигм є фундаментальним для роботи з Big Data. Проте важливо пам'ятати, що технології — це лише інструменти. Успішне застосування Big Data вимагає також розуміння предметної області, правильної постановки питань, критичного мислення при інтерпретації результатів та усвідомлення етичних наслідків використання даних.
